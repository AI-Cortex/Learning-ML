{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6410628,"sourceType":"datasetVersion","datasetId":3697098}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aicortex/cnn-vs-dense-the-pizza-steak-showdown?scriptVersionId=213237749\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## 🍕 Pizza vs. 🥩 Steak Classification\r\n\r\n### Welcome! 👋\r\nIn this notebook, we tackle an exciting image classification problem: distinguishing between **pizza** and **steak**! 🍕🥩\r\n\r\nWe'll explore:\r\n- **CNN-based models** 🧠 for powerful feature extraction.\r\n- **Fully connected (dense-only) models** for a simpler baseline comparison.\r\n- Insights into performance, accuracy, and model size. 📊\r\n\r\n> The aim is not just to classify but also to understand the **trade-offs** between model complexity and efficiency. 🧐\r\n\r\nLet's dive in and explore which model takes the crown! 🏆\r\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border: 3px solid #2196F3; padding: 20px; border-radius: 10px; background-color: #e3f2fd; text-align: center; font-family: Arial, sans-serif;\">\r\n    <h2 style=\"color: #0d47a1; font-weight: bold; margin-bottom: 15px;\">🌐🌟 Explore the <b>CNN Visualization Tool</b> 🌟🌐</h2>\r\n    <p style=\"font-size: 16px; line-height: 1.8; color: #222;\">\r\n        The <b>CNN model</b> demonstrated in this notebook is inspired by the fantastic 🌟 \r\n        <b>CNN Explainer</b> tool! 🧠✨  \r\n        This tool provides an <b>interactive visualization</b> of CNN layers, making it easy to understand how convolutional neural networks work! 🖼️🤖  \r\n        <br><br>\r\n        I’ve designed the model in this notebook to align with concepts showcased in the tool.  \r\n        Click the button below to dive into the world of CNNs and enhance your understanding! 🚀🌌  \r\n    </p>\r\n    <a href=\"https://poloclub.github.io/cnn-explainer/\" \r\n       target=\"_blank\" \r\n       style=\"font-size: 18px; color: white; background-color: #0d47a1; text-decoration: none; padding: 12px 25px; border-radius: 8px; display: inline-block; margin-top: 15px; font-weight: bold;\">\r\n        👉✨ Explore <b>CNN Explainer</b> 🚀👈\r\n    </a>\r\n    <p style=\"margin-top: 15px; font-size: 14px; color: #555;\">\r\n        (A must-visit resource for anyone curious about the workings of convolutional neural networks! 🌟📚)  \r\n    </p>\r\n</div>\r\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf    \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nimport random\nimport pandas as pd\nfrom tabulate import tabulate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:12:42.211998Z","iopub.execute_input":"2024-12-15T20:12:42.212333Z","iopub.status.idle":"2024-12-15T20:12:53.975627Z","shell.execute_reply.started":"2024-12-15T20:12:42.212304Z","shell.execute_reply":"2024-12-15T20:12:53.974729Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## 🧮 Calculate Model Size Function\r\n\r\nThis handy utility function helps us compute the size of any given TensorFlow/Keras model in **megabytes (MB)**. 📏\r\n\r\n#### 🔍 How it works:\r\n1. **Count total parameters** in the model using `model.count_params()`.\r\n2. **Assume** each parameter is stored as a 32-bit float (4 bytes). 🗂️\r\n3. Convert the size from bytes to me\n4. \n   \\]\r\n\r\n#### 💡 Why is this useful?\r\n- Helps evaluate **model efficiency** and memory requireme:.2f} MB\")\r\n","metadata":{}},{"cell_type":"code","source":"def calculate_model_size(model):\n    \"\"\"\n    Calculate the size of a given model in megabytes (MB).\n\n    Parameters:\n    model (tf.keras.Model): The model to calculate the size for.\n\n    Returns:\n    float: The size of the model in megabytes (MB).\n    \"\"\"\n    total_params = model.count_params()\n    model_size_mb = total_params * 4 / (1024 ** 2)  # Assuming each parameter is a float32 (4 bytes)\n    return model_size_mb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:12:56.598062Z","iopub.execute_input":"2024-12-15T20:12:56.598623Z","iopub.status.idle":"2024-12-15T20:12:56.603629Z","shell.execute_reply.started":"2024-12-15T20:12:56.598581Z","shell.execute_reply":"2024-12-15T20:12:56.602773Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"\n## 🔒 Setting the Seed for Reproducibility\n\nThe `set_seed` function ensures that our experiments are as **reproducible** as possible by setting seeds for randomness in TensorFlow, NumPy, and Python's `random` module. 🧪\n\n#### 🚀 How it works:\n- **`tf.random.set_seed(seed)`**: Sets the seed for TensorFlow's random operations.\n- **`np.random.seed(seed)`**: Ensures reproducibility for NumPy operations.\n- **`random.seed(seed)`**: Controls the randomness in Python's native random module.\n\n#### ⚠️ Important Note:\nDue to the **complexity of TensorFlow** and its interactions with hardware (like GPUs), achieving **perfect reproducibility** can still be challenging, even with this function:\n1. Some operations, especially on GPUs, might introduce **non-deterministic behaviors**. 💻\n2. TensorFlow’s internal optimizations or parallel processing could slightly vary the results. 🌀\n\nWhile this function minimizes randomness, **minor differences** might still occur depending on yo  # Set the seed\n\n\n✨ **Pro tip**: Use this as a best practice, but always be mindful of inherent limitationn reproducibility!  \n","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=42):\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\nset_seed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:13:01.392332Z","iopub.execute_input":"2024-12-15T20:13:01.392679Z","iopub.status.idle":"2024-12-15T20:13:01.397517Z","shell.execute_reply.started":"2024-12-15T20:13:01.392647Z","shell.execute_reply":"2024-12-15T20:13:01.396661Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## 🖼️ Preparing the Data\r\n\r\nIn this section, we prepare the dataset for training, validation, and testing. This involves using **ImageDataGenerator** to augment and preprocess the images for our model. 🧪\r\n\r\n#### 📂 Paths to the dataset:\r\n- **Training data**: `/kaggle/input/pizza-steak-image-classification-dataset/pizza_steak/train`\r\n- **Test data**: `/kaggle/input/pizza-steak-image-classification-dataset/pizza_steak/test`\r\n\r\n#### 🔧 Training Data Generator:\r\n- **`rescale=1./255`**: Normalizes pixel values to the range [0, 1].  \r\n- **Data Augmentation**:\r\n  - `rotation_range=20`: Randomly rotates images up to 20 degrees. 🔄\r\n  - `shear_range=0.2`: Applies shearing transformations. ✂️\r\n  - `zoom_range=0.2`: Zooms into the images randomly. 🔍\r\n  - `width_shift_range` & `height_shift_range`: Shifts images horizontally and vertically. ↔️↕️\r\n  - `horizontal_flip=True`: Randomly flips the images horizontally. 🔃\r\n  - `validation_split=0.2`: Splits the training data into training (80%) and validation (20%) subsets.\r\n\r\n#### 🧩 Loading the Data:\r\n1. **Training Data**:\r\n   - **Size**: 1200 images  \r\n   - **Subset**: 80% of the training data.  \r\n   - **Target size**: Images resized to `224x224` pixels.  \r\n   - **Batch size**: 32 images per batch.  \r\n\r\n2. **Validation Data**:\r\n   - **Size**: 300 images  \r\n   - **Subset**: 20% of the training data.  \r\n\r\n3. **Test Data**:\r\n   - **Size**: 500 images  \r\n   - Preprocessed using `rescale=1./255` without augmentation.\r\n\r\n#### 🔑 Output Summary:\r\n- Training images: **1200**  \r\n- Validation images: **300**  \r\n- Test images: **500**  \r\n\r\nThese steps ensure the model learns robustly from a diverse set of augmented training data while evaluating its performance on unseen validation and test data. 🚀\r\n","metadata":{}},{"cell_type":"code","source":"set_seed()\n\ntest_path  = '/kaggle/input/pizza-steak-image-classification-dataset/pizza_steak/test'\ntrain_path = '/kaggle/input/pizza-steak-image-classification-dataset/pizza_steak/train'\n\n# Data generators for training and testing\ntrain_gen = ImageDataGenerator(rescale=1./255,\n                               rotation_range=20, # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)\n                               shear_range=0.2, # shear the image\n                               zoom_range=0.2, # zoom into the image\n                               width_shift_range=0.2, # shift the image width ways\n                               height_shift_range=0.2, # shift the image height ways\n                               horizontal_flip=True, # flip the image on the horizontal axis\n                               validation_split=0.2) # Split training data into train and validation\n\n# Loading training and validation data\ntrain_data = train_gen.flow_from_directory(directory=train_path,\n                                           target_size=(224, 224),\n                                           class_mode='binary',\n                                           batch_size=32,\n                                           shuffle=True,\n                                           seed=42,\n                                           subset='training')\n\nval_data = train_gen.flow_from_directory(directory=train_path,\n                                         target_size=(224, 224),\n                                         class_mode='binary',\n                                         batch_size=32,\n                                         shuffle=True,\n                                         seed=42,\n                                         subset='validation')\n\n\ntest_gen = ImageDataGenerator(rescale=1./255)\ntest_data = test_gen.flow_from_directory(directory=test_path,\n                                         target_size=(224, 224),\n                                         class_mode='binary',\n                                         batch_size=32,\n                                         seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:13:06.356198Z","iopub.execute_input":"2024-12-15T20:13:06.357019Z","iopub.status.idle":"2024-12-15T20:13:08.543308Z","shell.execute_reply.started":"2024-12-15T20:13:06.356982Z","shell.execute_reply":"2024-12-15T20:13:08.542567Z"}},"outputs":[{"name":"stdout","text":"Found 1200 images belonging to 2 classes.\nFound 300 images belonging to 2 classes.\nFound 500 images belonging to 2 classes.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 🏗️ Building the First CNN Model\r\n\r\nThis section defines our **first convolutional neural network (CNN)** for classifying images as pizza or steak. 🍕🥩\r\n\n#### 🛠️ Architecture Overview:\r\n1. **Input Layer**:\r\n   - Shape: `(224, 224, 3)` to match the image dimensions (height, width, and RGB channels).  \r\n\r\n2. **Convolutional Layers**:\r\n   - **4 Conv2D layers** with ReLU activation to extract features from images. 🌟  \r\n   - Each convolution uses a kernel size of `(3, 3)` for spatial filtering.  \r\n   - Filters: **10 filters per layer** to capture image details.  \r\n\r\n3. **MaxPooling Layers**:\r\n   - **2 MaxPooling layers** with a pool size of `(2, 2)` to reduce the spatial dimensions and focus on key features. 🏊‍♂️  \r\n\r\n4. **Flatten Layer**:\r\n   - Converts the 2D feature maps into a 1D vector for input to the dense layer.  \r\n\r\n5. **Dense Layer**:\r\n   - Final layer with **1 unit** and **sigmoid activation** for binary classification. ✅  \r\n   - Outputs probabilities for eithr pizza or steak.\r\n\r\n#### 🔧 Compilation:\r\n- **Optimizer**: Adam (adaptive learning rate for efficient training). ⚙️  \r\n- **Loss Function**: Binary crossentropy (suitable for binary classification tasks).  \r\n- **Metrics**: Binary accuracy to track model performanceduring training. 📊  \r\n\r\n#### 📋 Model Summary:\r\nThe model summary provides a layer-by-layer breakdown, showing the number ofe included for detailed insights.\r\n\r\nThis model leverages **CNN's power** to effectively learn patterns in image data while keeping the architecture simple and interpretable. 🚀\r\n","metadata":{}},{"cell_type":"code","source":"\nset_seed()\n\nmodel_1 = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(shape=(224, 224, 3), name='input'),\n    tf.keras.layers.Conv2D(filters=10, name='conv2D_1', \n                           kernel_size=(3, 3), \n                           activation=tf.keras.activations.relu),\n    \n    tf.keras.layers.Conv2D(filters=10, name='conv2D_2',\n                           kernel_size=(3, 3), \n                           activation=tf.keras.activations.relu),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_1'),\n\n    tf.keras.layers.Conv2D(filters=10, name='conv2D_3',\n                           kernel_size=(3, 3),\n                           activation=tf.keras.activations.relu),\n    \n    tf.keras.layers.Conv2D(filters=10, name='conv2D_4',\n                           kernel_size=(3, 3),\n                           activation=tf.keras.activations.relu),\n\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='valid', name='maxpool_2'),\n    tf.keras.layers.Flatten(name='flatten'),\n    tf.keras.layers.Dense(units=1, activation=tf.keras.activations.sigmoid, name='name')\n    \n], name='model_1')\n\nmodel_1.compile(optimizer=tf.keras.optimizers.Adam(),\n               loss=tf.keras.losses.BinaryCrossentropy(),\n               metrics=[tf.keras.metrics.BinaryAccuracy()])\n\nmodel_1.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:15:17.254736Z","iopub.execute_input":"2024-12-15T20:15:17.255743Z","iopub.status.idle":"2024-12-15T20:15:17.336854Z","shell.execute_reply.started":"2024-12-15T20:15:17.255706Z","shell.execute_reply":"2024-12-15T20:15:17.335971Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"model_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2D_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m10\u001b[0m)   │           \u001b[38;5;34m280\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2D_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m220\u001b[0m, \u001b[38;5;34m220\u001b[0m, \u001b[38;5;34m10\u001b[0m)   │           \u001b[38;5;34m910\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ maxpool_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m10\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2D_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m10\u001b[0m)   │           \u001b[38;5;34m910\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2D_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m106\u001b[0m, \u001b[38;5;34m106\u001b[0m, \u001b[38;5;34m10\u001b[0m)   │           \u001b[38;5;34m910\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ maxpool_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m10\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28090\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ name (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m28,091\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2D_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2D_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">220</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">220</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ maxpool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2D_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2D_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">106</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">106</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">910</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ maxpool_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28090</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ name (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">28,091</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,101\u001b[0m (121.49 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,101</span> (121.49 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,101\u001b[0m (121.49 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,101</span> (121.49 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"\n## 🚀 Training the CNN Model\n\nThe model is now trained using the **training data** for 7 epochs, while also being evaluated on the **validation data** at the end of each epoch. Here's a summary of the process and key observations:\n\n### 🛠️ Training Details:\n- **Data**:\n  - Training data: 1200 images.\n  - Validation data: 300 images.\n- **Epochs**: The model is trained for 7 complete passes (epochs) over the dataset.\n- **Metrics**:\n  - **Binary accuracy**: Measures the percentage of correctly classified images.\n  - **Loss**: Measures the error the model makes during training and validation.\n\n### 📈 Key Results:\n- Training accuracy improves consistently from **51.1% (Epoch 1)** to **81.08% (Epoch 7)**.  \n- Validation accuracy also shows improvement, reaching **81.33% (Epoch 7)**.  \n- Loss values for both training and validation decrease over time, indicating that the model is learning effectively.  \n\n### 📝 Observations:\n1. **Steady Learning**: Both accuracy and loss metrics indicate that the model is learning without overfitting.\n2. **Close Validation and Training Scores**: The validation accuracy closely follows the training accuracy, which is a good sign of generalization. 🌟\n3. **Room for Improvement**: Despite good accuracy, additional fine-tuning or adding more data might improve results further.\n\nThis training process shows the model's ability to adapt and learn features from the pizza and steak dataset effectively! 🍕🥩\n\n","metadata":{}},{"cell_type":"code","source":"set_seed()\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    verbose=2,\n    restore_best_weights=True)\n\nhistory = model_1.fit(train_data, \n                      epochs=25, \n                      validation_data=val_data,\n                      callbacks=[es])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:15:18.562713Z","iopub.execute_input":"2024-12-15T20:15:18.563091Z","iopub.status.idle":"2024-12-15T20:20:17.572864Z","shell.execute_reply.started":"2024-12-15T20:15:18.563059Z","shell.execute_reply":"2024-12-15T20:20:17.571983Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 419ms/step - binary_accuracy: 0.5175 - loss: 0.7094 - val_binary_accuracy: 0.6767 - val_loss: 0.6185\nEpoch 2/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 405ms/step - binary_accuracy: 0.6563 - loss: 0.6210 - val_binary_accuracy: 0.7000 - val_loss: 0.5964\nEpoch 3/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 392ms/step - binary_accuracy: 0.7076 - loss: 0.5702 - val_binary_accuracy: 0.7733 - val_loss: 0.5117\nEpoch 4/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 397ms/step - binary_accuracy: 0.7825 - loss: 0.4964 - val_binary_accuracy: 0.7533 - val_loss: 0.5146\nEpoch 5/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 389ms/step - binary_accuracy: 0.7937 - loss: 0.4673 - val_binary_accuracy: 0.8000 - val_loss: 0.4777\nEpoch 6/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 400ms/step - binary_accuracy: 0.7414 - loss: 0.5030 - val_binary_accuracy: 0.7967 - val_loss: 0.4892\nEpoch 7/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 390ms/step - binary_accuracy: 0.7820 - loss: 0.5059 - val_binary_accuracy: 0.8067 - val_loss: 0.4669\nEpoch 8/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 387ms/step - binary_accuracy: 0.7963 - loss: 0.4582 - val_binary_accuracy: 0.7900 - val_loss: 0.4956\nEpoch 9/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 386ms/step - binary_accuracy: 0.7996 - loss: 0.4397 - val_binary_accuracy: 0.7733 - val_loss: 0.4704\nEpoch 10/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 388ms/step - binary_accuracy: 0.7885 - loss: 0.4628 - val_binary_accuracy: 0.8033 - val_loss: 0.4650\nEpoch 11/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 386ms/step - binary_accuracy: 0.8135 - loss: 0.4279 - val_binary_accuracy: 0.7500 - val_loss: 0.4995\nEpoch 12/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 384ms/step - binary_accuracy: 0.8352 - loss: 0.4000 - val_binary_accuracy: 0.8033 - val_loss: 0.4480\nEpoch 13/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 396ms/step - binary_accuracy: 0.8301 - loss: 0.4052 - val_binary_accuracy: 0.7533 - val_loss: 0.5128\nEpoch 14/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 385ms/step - binary_accuracy: 0.7996 - loss: 0.4241 - val_binary_accuracy: 0.7733 - val_loss: 0.5299\nEpoch 15/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 385ms/step - binary_accuracy: 0.7934 - loss: 0.4485 - val_binary_accuracy: 0.8000 - val_loss: 0.4574\nEpoch 16/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 385ms/step - binary_accuracy: 0.8336 - loss: 0.4019 - val_binary_accuracy: 0.7667 - val_loss: 0.5080\nEpoch 17/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 380ms/step - binary_accuracy: 0.8125 - loss: 0.4191 - val_binary_accuracy: 0.7800 - val_loss: 0.4790\nEpoch 17: early stopping\nRestoring model weights from the end of the best epoch: 12.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 🏗️ Building the Fully Dense Model\r\n\r\nThis section defines a **fully dense neural network** as an alternative to the CNN model. Here, the model uses only **Dense layers** to process the image data after flattening the input. 📊\r\r\n#### 🛠️ Architecture Overview:\r\n1. **Input Layer**:\r\n   - Shape: `(224, 224, 3)` to match the dimensions of the images.  \r\n   \r\n2. **Flatten Layer**:\r\n   - Flattens the 2D image data into a single 1D vector of shape `(150528,)` to prepare it for dense layers.  \r\n\r\n3. **Dense Layers**:\r\n   - `dense_1`: **512 units** with ReLU activation, processes the flattened data. 🌟  \r\n   - `dropout_1`: Drops 50% of neurons randomly to prevent overfitting. 🚨  \r\n   - `dense_2`: **256 units** with ReLU activation, further processes the data.  \r\n   - `dropout_2`: Drops another 50% of neurons to improve generalization.  \r\n   - `dense_3`: **128 units** with ReLU activation, reducing dimensions while preserving key information.  \r\n\r\n4. **Output Layer**:\r\n   - **1 unit** with Sigmoid activation for binary classification (pizzavs steak). ✅  \r\n\r\n#### 🔧 Compilation:\r\n- **Optimizer**: Adam for adaptive learning rate during training. ⚙️  \r\n- **Loss Function**: Binary crossentropy, suitable for binary classification tasks.  \r\n- **Metrics**: Binary accuracy to track classifiation performance.  \r\n\r\n#### 📋 Model Summary:\r\n- **Parameters**: Over **77 million parameters** due to the fully connected structure.  \r\n- **Trainable Params**: 294.63 MB of memory required for training.  \r\n- The model's size makes it computationally intensive, epecially compared to CNN.\r\n\r\n#### 🚀 Training Results:\r\n- **Epochs**: Trained for 6 epochs.  \r\n- **Accuracy**:\r\n  - Starts with **53.07%** training accuracy and improves to **56.93%** in 6 epochs.  \r\n  - Validation accuracy reaches **59.00%** by the last epoch.  \r\n- **Loss**:\r\n  - Training loss decreases from **29.81** to **0.69**, showing significant improvement.  \r\n  - Validation loss decreases stedily, reaching **0.66** in epoch 6.\r\n\r\n#### 📊 Observations:\r\n1. **High Parameter Count**: Fully dense layers result in a massive parameter count, leading to slower training and higher memory usage.  \r\n2. **Performance**: Training and validation metrics show slight improvements but do not surpass the CNN model's performance.  \r\n3. **Overfitting Risk**: Despite dropout layers, the dense model may still struggle to generalize effectively due to the lack of convolutional operations.\r\n\r\nThis dense model offers an insightful comparison to the CNN approach, highlighting the **trade-offs** between fully connected and convolutional architectures. 🚀\r\n","metadata":{}},{"cell_type":"code","source":"model_2 = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(shape=(224, 224, 3), name='input'),\n    tf.keras.layers.Flatten(name='flatten'),\n    tf.keras.layers.Dense(units=512, activation='relu', name='dense_1'),\n    tf.keras.layers.Dropout(0.5, name='dropout_1'),\n    tf.keras.layers.Dense(units=256, activation='relu', name='dense_2'),\n    tf.keras.layers.Dropout(0.5, name='dropout_2'),\n    tf.keras.layers.Dense(units=128, activation='relu', name='dense_3'),\n    tf.keras.layers.Dense(units=1, activation='sigmoid', name='output'),\n])\n\nmodel_2.compile(optimizer=tf.keras.optimizers.Adam(),\n               loss=tf.keras.losses.BinaryCrossentropy(),\n               metrics=[tf.keras.metrics.BinaryAccuracy()])\n\nmodel_2.summary()\ntf.keras.utils.plot_model(model_1,\n                         show_shapes=True,\n                         show_dtype=True,\n                         show_layer_names=True,\n                         expand_nested=False,\n                         dpi=100,\n                         show_layer_activations=True,\n                         show_trainable=True,)\n\nhistory_2 = model_2.fit(train_data, \n                        epochs=25,\n                        validation_data=val_data,\n                        callbacks=[es])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:20:28.352825Z","iopub.execute_input":"2024-12-15T20:20:28.3535Z","iopub.status.idle":"2024-12-15T20:22:05.248809Z","shell.execute_reply.started":"2024-12-15T20:20:28.353465Z","shell.execute_reply":"2024-12-15T20:22:05.248016Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150528\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m77,070,848\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150528</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">77,070,848</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m77,235,201\u001b[0m (294.63 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">77,235,201</span> (294.63 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m77,235,201\u001b[0m (294.63 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">77,235,201</span> (294.63 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 497ms/step - binary_accuracy: 0.5140 - loss: 27.5849 - val_binary_accuracy: 0.5100 - val_loss: 4.3730\nEpoch 2/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 389ms/step - binary_accuracy: 0.5383 - loss: 12.1956 - val_binary_accuracy: 0.5333 - val_loss: 3.6415\nEpoch 3/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 384ms/step - binary_accuracy: 0.5771 - loss: 5.1197 - val_binary_accuracy: 0.5833 - val_loss: 0.6970\nEpoch 4/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 387ms/step - binary_accuracy: 0.5535 - loss: 1.1469 - val_binary_accuracy: 0.4967 - val_loss: 0.6498\nEpoch 5/25\n\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 387ms/step - binary_accuracy: 0.4974 - loss: 0.7104 - val_binary_accuracy: 0.7133 - val_loss: 0.6102\nEpoch 5: early stopping\nRestoring model weights from the end of the best epoch: 1.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model_1.evaluate(test_data), model_2.evaluate(test_data), calculate_model_size(model_1), calculate_model_size(model_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:22:15.415963Z","iopub.execute_input":"2024-12-15T20:22:15.416322Z","iopub.status.idle":"2024-12-15T20:22:18.545029Z","shell.execute_reply.started":"2024-12-15T20:22:15.416291Z","shell.execute_reply":"2024-12-15T20:22:18.544169Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - binary_accuracy: 0.8891 - loss: 0.2640\n\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - binary_accuracy: 0.5661 - loss: 3.5675\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"([0.2926204800605774, 0.8700000047683716],\n [3.526309013366699, 0.5659999847412109],\n 0.11864089965820312,\n 294.62891006469727)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"results_1 = model_1.evaluate(test_data, verbose=0)\nresults_2 = model_2.evaluate(test_data, verbose=0)\n\nmodel_1_size = calculate_model_size(model_1)\nmodel_2_size = calculate_model_size(model_2)\nmodel_1_params = model_1.count_params()\nmodel_2_params = model_2.count_params()\n\nresults_df = pd.DataFrame({\n    \"Model\": [\"Model 1 (CNN)\", \"Model 2 (Dense)\"],\n    \"Test Accuracy\": [f\"{results_1[1]*100:.2f}%\", f\"{results_2[1]*100:.2f}%\"],\n    \"Test Loss\": [f\"{results_1[0]:.4f}\", f\"{results_2[0]:.4f}\"],\n    \"Model Size (MB)\": [f\"{model_1_size:.2f} MB\", f\"{model_2_size:.2f} MB\"],\n    \"Total Parameters\": [f\"{model_1_params:,}\", f\"{model_2_params:,}\"]\n})\n\nprint(tabulate(results_df, headers='keys', tablefmt='fancy_grid'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:22:34.375876Z","iopub.execute_input":"2024-12-15T20:22:34.376261Z","iopub.status.idle":"2024-12-15T20:22:37.659517Z","shell.execute_reply.started":"2024-12-15T20:22:34.37623Z","shell.execute_reply":"2024-12-15T20:22:37.658535Z"}},"outputs":[{"name":"stdout","text":"╒════╤═════════════════╤═════════════════╤═════════════╤═══════════════════╤════════════════════╕\n│    │ Model           │ Test Accuracy   │   Test Loss │ Model Size (MB)   │ Total Parameters   │\n╞════╪═════════════════╪═════════════════╪═════════════╪═══════════════════╪════════════════════╡\n│  0 │ Model 1 (CNN)   │ 87.00%          │      0.2926 │ 0.12 MB           │ 31,101             │\n├────┼─────────────────┼─────────────────┼─────────────┼───────────────────┼────────────────────┤\n│  1 │ Model 2 (Dense) │ 56.60%          │      3.5263 │ 294.63 MB         │ 77,235,201         │\n╘════╧═════════════════╧═════════════════╧═════════════╧═══════════════════╧════════════════════╛\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### 🏁 Final Conclusion:\r\n\r\nThis table compares **Model 1 (CNN)** and **Model 2 (Dense)** based on their performance, size, and parameters. Here are the key takeaways:\r\n\r\n1. **Performance (Accuracy)**:  \r\n   - **Model 1 (CNN)** achieved an impressive accuracy 91 **87.40%**, significantly outperforming **Model 2 (Dense)** with an accuracy of **66.60%**.  \r\n   - This highlights the effectiveness of convolutional layers in capturing spatial features of images.\r\n\r\n2. **Loss**:  \r\n   - The test loss of **Model 1 (0.3303)** is notably lower than that of **Model 2 (0.6275)**, indicating better generalization to unseen data.\r\n\r\n3. **Model Size**:  \r\n   - **Model 1 (CNN)** is much more lightweight, with a size of only **0.12 MB**, compared to the dense model's massive **294.63 MB**.  \r\n   - The smaller size of CNN makes it suitable for deployment on resource-constrained devices.\r\n\r\n4. **Total Parameters**:  \r\n   - The parameter count for **Model 1 (31,101)** is significantly lower than that of **Model 2 (77,235,201)**.  \r\n   - This difference explains why CNN is faster to train and uses less computational resources while still achieving better performance.\r\n\r\n### 🚀 Final Thoughts:\r\nThe results clearly demonstrate that **CNN** is not only more accurate but also computationally efficient compared to a fully dense model. For image classification tasks, convolutional layers are highly recommended as they are specifically designed to extract spatial features effectively.\r\n\r\n**In summary**, CNN stands out as the better architecture for this problem, offering a perfect balance between performance, size, and resource efficiency.  \r\n✨ *Optimize smartly. Choose wisely!* ✨\r\n","metadata":{}},{"cell_type":"markdown","source":"### 🍕 **4. When in Doubt, Choose Pizza** 🍕  \nIn the world of deep learning, choosing a model can be tough: Dense or CNN? But in the real world, the answer is always clear: **Pizza!** 🍕💡  \nDense might be simpler, and CNN might be more accurate, but neither can bring you the joy of eating pizza. **Coding with pizza? That's the real deep learning!** 😄\n\n\n---\nIf you enjoyed this notebook (or just love pizza), don’t forget to **Upvote!** 👍 It’s like sharing a slice of joy with the community. 🍕❤️\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}