{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9243,"sourceType":"datasetVersion","datasetId":2243}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aicortex/ideal-learning-rate-for-fashion-mnist?scriptVersionId=208204455\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"border-radius:10px;\r\n            border:#0b0265 solid;\r\n           background-color:#e8efff;\r\n           font-size:110%;\r\n           letter-spacing:0.5px;\r\n            text-align: center\">\r\n\r\n<center><h1 style=\"padding: 25px 0px; color:#0b0265; font-weight: bold; font-family: Cursive\">\r\n📊 Exploring the Ideal Learning Rate for Fashion MNIST 🧥👖👗</h1></></center>     \r\n\r\n</div>","metadata":{}},{"cell_type":"markdown","source":"<center style=\"font-family:cursive; font-size:18px; color:#eb4634;\">This notebook focuses on a crucial aspect of deep learning: finding the optimal learning rate for training a neural network. The learning rate is a key hyperparameter that significantly affects the model's performance and training efficiency. Choosing an ideal value ensures faster convergence and prevents underfitting or overfitting.</center>","metadata":{}},{"cell_type":"markdown","source":"## 🗂️ Notebook Contents:\n- **Data Preparation**:  \n   The `Fashion MNIST` dataset is loaded and preprocessed to prepare it for training. This dataset contains 28x28 grayscale images of 10 clothing categories like T-shirts, trousers, dresses, and more. 🛍️\n   \n- **Model Building**:  \n   A deep learning model is constructed using TensorFlow and Keras. The architecture is simple yet effective for our goal.\n\n- **Learning Rate Exploration**:  \n   Different learning rates are experimented with to observe their effects on loss values. Visualization is a key part of understanding this process. 📈\n\n- **Training and Evaluation**:  \n   The model is trained using the best learning rate identified, and its performance is evaluated on the test set. Metrics like accuracy and loss are logged.\n\n- **Confusion Matrix Analysis**:  \n   A confusion matrix is generated to analyze the model's predictions. This step helps understand where the model performs well and where it struggles.\n\n- **Final Observations and Insights**:  \n   Based on the results, the notebook provides insights into how the learning rate impacts model training and what values work best for this dataset.\n\n👩‍💻 **Purpose of the Notebook**:  \nTo understand the importance of selecting an ideal learning rate, practice hyperparameter tuning, and visualize how it impacts training dynamics. By the end, you'll gain a deeper understanding of how to improve model performance efficiently.\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.datasets import fashion_mnist\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport random\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1️⃣ Loading the Dataset\r\n\r\n\r\n\r\n## 🛍️ What does this code do?  \r\n1. **Load the Dataset**:  \r\n   This line loads the `Fashion MNIST` dataset using TensorFlow's `fashion_mnist.load_data()` method.  \r\n   - If the dataset is not already available locally, it downloads it from TensorFlow's servers.\r\n\r\n2. **Structure**:  \r\n   - The dataset contains grayscale images of clothing items belonging to 10 classes. Each image has a size of 28x28 pixels.\r\n   - Classes include categories like T-shirt/top, Trouser, Pullover, and more.\r\n\r\n3. **Output**:  \r\n   - Once loaded, the dataset is ready for preprocessing and splitting into training and testing sets.\r\n","metadata":{}},{"cell_type":"code","source":"data = fashion_mnist.load_data()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2️⃣ Splitting and Inspecting the Dataset\n\n## 🔍 What does this code do?  \n1. **Split the Data**:  \n   - The loaded data is unpacked into four variables:\n     - `train_data`: The images for training.\n     - `train_label`: Labels corresponding to the training images.\n     - `test_data`: The images for testing.\n     - `test_label`: Labels corresponding to the testing images.\n\n2. **Check the Shapes**:  \n   - Training Data Shape: `(60000, 28, 28)`  \n     This indicates 60,000 grayscale images of size 28x28 for training.  \n   - Training Labels Shape: `(60000,)`  \n     Each image is associated with one label.  \n   - Testing Data Shape: `(10000, 28, 28)`  \n     This indicates 10,000 grayscale images of size 28x28 for testing.  \n   - Testing Labels Shape: `(10000,)`  \n     Each testing image has a corresponding label.\n\n## ✨ Why is this important?  \n- Understanding the shape and dimensions of the dataset ensures that we can properly configure our model's input and output layers.  \n- It also helps confirm that the data is correctly split into training and testing sets. ✅","metadata":{}},{"cell_type":"code","source":"(train_data, train_label), (test_data, test_label) = data\n(train_data.shape, train_label.shape), (test_data.shape, test_label.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3️⃣ Visualizing the Dataset 🖼️✨\r\n\r\nThis code is just to **get a feel for the data**! By randomly picking and plotting some images, we can see what kind of items (like T-shirts, shoes, or bags) we're working with. It's not necessary for training, but it helps us connect with the dataset visually. Cool, right? 😎👕👜👟","metadata":{}},{"cell_type":"code","source":"label_name = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nseq = range(len(train_data))\nplt.figure(figsize=(15, 8))\nfor i in range(8):\n    plt.subplot(2, 4, i+1)\n    index = random.choice(seq)\n    plt.imshow(train_data[index], cmap=plt.cm.binary)\n    plt.title(str(label_name[train_label[index]]))\n    plt.axis(False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4️⃣ Building a Simple Model to Test Learning Rates 🧪⚡\r\n\r\nHere, we're creating a **simplified version of the final model** to experiment with different learning rates. The idea is to:\r\n- Keep the model architecture similar to the final one.\r\n- Use fewer neurons per layer so the model trains **faster** during the 50 epochs.  \r\nThis is especially useful when trying to find the optimal learning rate, as it saves us time without compromising the experiment's effectiveness. ⏱️💡\r\n\r\n---\r\n\r\n### 🌟 Advantages:\r\n- **Faster Training**: By reducing the number of neurons, we minimize the computational load. 🚀  \r\n- **Same Structure**: It mirrors the final architecture, so the insights from this experiment will apply directly to the larger model. 🔄\r\n\r\n---\r\n\r\n### ⚠️ Disadvantages:\r\n- **Limited Capacity**: With fewer neurons, the model might not learn complex patterns well. 🤔  \r\n- **Not Final Results**: This is just for testing; the performance may vary when using the full model. 🎛️\r\n\r\n---\r\n\r\nThe key takeaway? **Quick experimentation = smarter optimization!** 🎯✨\r\n","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(shape=(28, 28), name='input'),\n    tf.keras.layers.Flatten(name='flatten'),\n    tf.keras.layers.Dense(units=10, activation='relu', name='hidden_1'),\n    tf.keras.layers.Dense(units=10, activation='relu', name='hidden_2'),\n    tf.keras.layers.Dense(units=10, activation='relu', name='hidden_3'),\n    tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax, name='output'),   \n], name='model')\n\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n             )\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))\nhistory = model.fit(train_data, train_label, epochs=50,\n         validation_data=(test_data, test_label),\n        callbacks=[lr_scheduler])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5️⃣ Plotting Learning Rate vs. Loss 📉⚡\n\n### Step-by-Step:\n\n1. **X-axis (Learning Rate)**:  \n   We varied the learning rate exponentially from $(10^{-4})$ to $(10^{-2})$.\n\n2. **Y-axis (Loss)**:  \n   This shows how the loss changes with different learning rates.\n\n3. **Optimal Point**:  \n   - The **ideal learning rate** is typically found **just before** the loss starts to increase sharply.\n   - In this plot, the loss decreases until around $(10^{-3})$ and then starts to rise.\n   - Therefore, a learning rate slightly **before** $(10^{-3})$, such as $(10^{-3.5})$ or $(3 \\times 10^{-4})$, is often optimal.\n\n### Why is this important?  \nChoosing the optimal learning rate ensures the model learns efficiently without overshooting or converging too slowly. 🚀✨\n\n**Note**: It's advisable to select a learning rate where the loss is decreasing and hasn't started to increase, indicating the model is learning effectively without instability. ([walkwithfastai.com](https://walkwithfastai.com/lr_finder))\n","metadata":{}},{"cell_type":"code","source":"lr = 1e-4 * 10**(np.arange(50)/20)\nplt.semilogx(lr, history.history['loss'])\nplt.xlabel(\"Learning rate\")\nplt.ylabel(\"Loss\")\nplt.title(\"Finding the ideal learning rate\");","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6️⃣ Visualizing Training Progress 📊✨\r\n\r\n### What does this plot show?  \r\nThis plot visualizes key metrics over 50 epochs, giving us a detailed view of the model's training and validation performance:\r\n\r\n1. **Blue Line (Loss)**:  \r\n   - Tracks how the training loss decreases over time.\r\n   - A steady decrease shows the model is learning.  \r\n   - Any sudden jumps might indicate instability.\r\n\r\n2. **Green Line (Validation Loss)**:  \r\n   - Tracks the loss on validation data.\r\n   - The goal is for this to follow a similar trend to training loss. If it diverges significantly, it could mean overfitting.  \r\n\r\n3. **Orange Line (Sparse Categorical Accuracy)**:  \r\n   - Tracks the training accuracy improving as the model learns.\r\n\r\n4. **Red Line (Validation Accuracy)**:  \r\n   - Tracks how well the model generalizes to unseen data.\r\n\r\n5. **Purple Line (Learning Rate)**:  \r\n   - Indicates the learning rate used during each epoch.\r\n\r\n### Key Observations:  \r\n- The training and validation loss decrease initially, which is a good sign.  \r\n- Validation accuracy stabilizes, showing the learning rate strategy worked well.  \r\n- If the validation loss starts to increase while accuracy stagnates, it may suggest **overfitting**. Keep an eye on this trend! 👀\r\n\r\n### Why is this important?  \r\nThis combined view lets us evaluate both **learning efficiency** and **generalization performance** in one glance, helping fine-tune the model and hyperparameters effectively. 🚀📈\r\n","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7️⃣ Building the Final Model 🛠️🧠\r\n\r\n### What's happening here?  \r\n1. **Model Structure**:  \r\n   - This is the **final model**, built with more complex architecture compared to the initial one.  \r\n   - It has:\r\n     - **Two hidden layers** with 128 neurons each.\r\n     - **One smaller hidden layer** with 64 neurons.\r\n     - An output layer with 10 neurons (for 10 classes).\r\n\r\n2. **Activation Functions**:  \r\n   - **ReLU** is used in hidden layers for non-linearity and efficient training.\r\n   - **Softmax** in the output layer ensures the output is a probability distribution.\r\n\r\n3. **Compilation**:  \r\n   - **Loss**: Sparse Categorical Crossentropy for multi-class classification.\r\n   - **Optimizer**: Adam for efficient gradient-based optimization.\r\n   - **Metrics**: Sparse Categorical Accuracy to track training progress.\r\n\r\n4. **Visualization**:  \r\n   - `model.summary()` provides a detailed textual view of the model's architecture.  \r\n   - `plot_model()` generates a visual diagram showing layers, shapes, and activations. 🎨\r\n\r\n---\r\n\r\n### Why is this important?  \r\nThis model is **designed for better performance** compared to the simpler model. It uses more neurons to capture complex patterns in the data, making it suitable for final training and evaluation. 🚀✨\r\n","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\n\nmodel_2 = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(shape=(28, 28), name='input'),\n    tf.keras.layers.Flatten(name='flatten'),\n    tf.keras.layers.Dense(units=128, activation='relu', name='hidden_1'),\n    tf.keras.layers.Dense(units=128, activation='relu', name='hidden_2'),\n    tf.keras.layers.Dense(units=64, activation='relu', name='hidden_3'),\n    tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax, name='output'),   \n], name='model_2')\n\nmodel_2.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n             )\n\nmodel_2.summary()\nplot_model(model_2, show_shapes=True, show_dtype=True, show_layer_names=True,\n           expand_nested=True, dpi=100, show_layer_activations=True,show_trainable=True,)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8️⃣ Training, Evaluation, and Predictions 🚀📊\r\n","metadata":{}},{"cell_type":"markdown","source":"### 1️⃣ Training the Model\r\n\r\n- The model is trained for **10 epochs** with a **15% validation split**.\r\n- **Key Observations**:\r\n  - Training accuracy improves steadily across epochs.\r\n  - Validation loss and accuracy also improve, indicating good generalization.\r\n","metadata":{}},{"cell_type":"code","source":"history = model_2.fit(train_data, train_label, epochs=10,\n         validation_split=0.15)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2️⃣ Evaluating the Model\r\n\r\n- **Loss**: 0.4345  \r\n- **Accuracy**: 84.83%  \r\n- The evaluation shows that the model performs reasonably well on unseen test data. ✅\r\n","metadata":{}},{"cell_type":"code","source":"model_2.evaluate(test_data, test_label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3️⃣ Making Predictions\r\n\r\n- **Shape of Predictions**: `(10000, 10)`  \r\n  - Each row represents the probabilities for the 10 classes.  \r\n- **Argmax**: Picks the class with the highest probability for each input.\r\n","metadata":{}},{"cell_type":"code","source":"prob = model_2.predict(test_data)\nprint('shape : ', prob.shape)\nprint(prob)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9️⃣ Confusion Matrix Analysis 🎯📊\r\n\r\n### What does this show?  \r\nThe **confusion matrix** provides a detailed breakdown of the model's predictions vs. actual labels. It helps us understand where the model performs well and where it struggles.\r\n\r\n---\r\n\r\n### Key Insights:\r\n1. **Diagonal Values**:  \r\n   - These represent correctly classifiedT-shirts).  \r\n   - Higher diagonal values = better performance! ✅\r\n\r\n2. **Off-Diagonal Values**:  \r\n   - These indicate misclas Pullovers).  \r\n   - Look for classes with high misclassification rates to identify where the model struggles.\r\n\r\n3. **Class-Wise Performance**:  \r\n   - For example, **Trousers** have 967 correct predictions and very few misclassifications, showing the model excels in this class. 👖  \r\n   - However, **Pullovers** and **Dresses** show higher misclassification, which could mean similar features between those classes. 🧥👗\r\n\r\n---\r\n\r\n### Why use a confusion matrix?  \r\n- It goes beyond simple accuracy by giving a **class-level performance breakdown**.  \r\n- This helps identify which classes need more attention (e.g., better preprocessing or more training data).  \r\n- Visualizing it makes debugging and improvements easier! 🔍✨\r\n","metadata":{}},{"cell_type":"code","source":"pred = tf.argmax(prob, axis=1)\npred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(test_label, pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=label_name)\n\nfig, ax = plt.subplots(figsize=(15, 8))\ndisp.plot(ax=ax)\n\nplt.xticks(rotation=90)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🔟 Final Thoughts on the Model 🧠✨\n\nWell, it turns out that the **default learning rate** in the Adam optimizer was already working like a charm! 🎩✨ Maybe Adam really knows what's best, huh? 😂 But hey, testing learning rates wasn’t a waste—we confirmed what works and learned something along the way. Knowledge is power, right? 💪\n\n---\n\n### The Verdict:  \nThis model did **great** for this notebook! 🏆 It performed well on most classes and gave us a solid foundation. But let’s not fool ourselves—it’s not the ultimate solution. There’s always room for improvement, whether that’s tweaking the architecture, adding more data, or experimenting with new ideas. 🚀\n\n---\n\n### What’s Next?  \nIn the **next notebook**, we’ll dive deeper into **more advanced optimizations** and test some new tricks to push this model to its limits. Stay curious and excited for what’s to come! 😎✨\n\n---\n\n### If You Found This Helpful…  \nFeel free to **upvote** 👍 and share your thoughts or questions! I’d love to hear your feedback, and maybe we can collaborate on even cooler ideas. Let’s keep the learning alive! 🧑‍💻🌟\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}